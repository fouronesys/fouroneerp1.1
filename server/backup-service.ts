import { db, pool } from "./db";
import { storage } from "./storage";
import { auditLogger } from "./audit-logger";
import fs from 'fs';
import path from 'path';
import { exec } from 'child_process';
import { promisify } from 'util';

const execAsync = promisify(exec);

export interface BackupRecord {
  id: string;
  name: string;
  type: 'full' | 'incremental' | 'rnc_only';
  description: string;
  size: number;
  createdAt: Date;
  createdBy: string;
  path: string;
  isCorrupted?: boolean;
  metadata?: {
    tableCount: number;
    recordCount: number;
    checksum: string;
  };
}

export class BackupService {
  private backupDir = './downloads/backups';
  private tempDir = './downloads/temp';

  constructor() {
    this.ensureDirectories();
  }

  private ensureDirectories() {
    if (!fs.existsSync(this.backupDir)) {
      fs.mkdirSync(this.backupDir, { recursive: true });
    }
    if (!fs.existsSync(this.tempDir)) {
      fs.mkdirSync(this.tempDir, { recursive: true });
    }
  }

  async createFullBackup(userId: string, description: string = "Automated full backup"): Promise<BackupRecord> {
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const backupName = `backup_full_${timestamp}.sql`;
    const backupPath = path.join(this.backupDir, backupName);

    try {
      // Generate metadata header first
      const metadata = await this.generateMetadata();
      const headerContent = `-- Four One Solutions ERP Database Backup
-- Generated: ${new Date().toISOString()}
-- Type: full
-- Description: ${description}
-- Generated by: ${userId}
-- Tables: ${metadata.tableCount}
-- Records: ${metadata.recordCount}
-- Checksum: ${metadata.checksum}

`;

      // Write header to file
      fs.writeFileSync(backupPath, headerContent);
      
      // Stream pg_dump output directly to file to avoid memory issues
      const pgDumpCommand = `pg_dump "${process.env.DATABASE_URL}" --no-owner --no-privileges >> "${backupPath}"`;
      
      // Use spawn instead of exec to stream output
      await new Promise((resolve, reject) => {
        const child = require('child_process').spawn('sh', ['-c', pgDumpCommand], {
          stdio: 'inherit'
        });
        
        child.on('exit', (code: number | null) => {
          if (code === 0) {
            resolve(null);
          } else {
            reject(new Error(`pg_dump exited with code ${code}`));
          }
        });
        
        child.on('error', reject);
      });
      
      const stats = fs.statSync(backupPath);
      
      const backupRecord: BackupRecord = {
        id: timestamp,
        name: backupName,
        type: 'full',
        description,
        size: stats.size,
        createdAt: new Date(),
        createdBy: userId,
        path: backupPath,
        metadata
      };

      await auditLogger.log({
        userId,
        module: 'backup',
        action: "backup_created",
        entityType: "backup",
        entityId: backupName,
        newValues: { 
          type: 'full', 
          description, 
          size: backupRecord.size,
          tableCount: metadata.tableCount,
          recordCount: metadata.recordCount
        },
        timestamp: new Date(),
        success: true,
        severity: 'info'
      });

      return backupRecord;
    } catch (error) {
      console.error("Error creating backup:", error);
      throw new Error(`Failed to create backup: ${error}`);
    }
  }

  async createIncrementalBackup(userId: string, since: Date): Promise<BackupRecord> {
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const backupName = `backup_incremental_${timestamp}.json`;
    const backupPath = path.join(this.backupDir, backupName);

    try {
      // Get changes since last backup
      const changes = await this.getIncrementalChanges(since);
      
      const backupContent = {
        metadata: {
          type: 'incremental',
          created: new Date().toISOString(),
          since: since.toISOString(),
          generatedBy: userId
        },
        changes
      };

      fs.writeFileSync(backupPath, JSON.stringify(backupContent, null, 2));
      
      const stats = fs.statSync(backupPath);
      
      const backupRecord: BackupRecord = {
        id: timestamp,
        name: backupName,
        type: 'incremental',
        description: `Incremental backup since ${since.toISOString()}`,
        size: stats.size,
        createdAt: new Date(),
        createdBy: userId,
        path: backupPath
      };

      await auditLogger.log({
        userId,
        module: 'backup',
        action: "backup_created",
        entityType: "backup",
        entityId: backupName,
        newValues: { 
          type: 'incremental', 
          since: since.toISOString(),
          size: backupRecord.size 
        },
        timestamp: new Date(),
        success: true,
        severity: 'info'
      });

      return backupRecord;
    } catch (error) {
      console.error("Error creating incremental backup:", error);
      throw new Error(`Failed to create incremental backup: ${error}`);
    }
  }

  async listBackups(): Promise<BackupRecord[]> {
    const backups: BackupRecord[] = [];
    
    if (!fs.existsSync(this.backupDir)) {
      return backups;
    }

    const files = fs.readdirSync(this.backupDir);
    
    for (const file of files) {
      try {
        const filePath = path.join(this.backupDir, file);
        const stats = fs.statSync(filePath);
        
        const backup: BackupRecord = {
          id: file.replace(/\.(sql|json)$/, ''),
          name: file,
          type: this.determineBackupType(file),
          description: await this.getBackupDescription(filePath),
          size: stats.size,
          createdAt: stats.birthtime,
          createdBy: 'system',
          path: filePath,
          isCorrupted: await this.checkBackupIntegrity(filePath)
        };
        
        backups.push(backup);
      } catch (error) {
        console.error(`Error processing backup file ${file}:`, error);
      }
    }
    
    return backups.sort((a, b) => b.createdAt.getTime() - a.createdAt.getTime());
  }

  async restoreFromBackup(backupId: string, userId: string): Promise<boolean> {
    try {
      const backups = await this.listBackups();
      const backup = backups.find(b => b.id === backupId);
      
      if (!backup) {
        throw new Error(`Backup ${backupId} not found`);
      }

      if (backup.isCorrupted) {
        throw new Error(`Backup ${backupId} is corrupted and cannot be restored`);
      }

      if (backup.type === 'full') {
        await this.restoreFullBackup(backup.path);
      } else if (backup.type === 'incremental') {
        await this.restoreIncrementalBackup(backup.path);
      }

      await auditLogger.log({
        userId,
        module: 'backup',
        action: "backup_restored",
        entityType: "backup", 
        entityId: backupId,
        newValues: { 
          backupType: backup.type,
          backupSize: backup.size,
          backupDate: backup.createdAt.toISOString()
        },
        timestamp: new Date(),
        success: true,
        severity: 'info'
      });

      return true;
    } catch (error) {
      console.error("Error restoring backup:", error);
      throw new Error(`Failed to restore backup: ${error}`);
    }
  }

  async exportBackup(backupId: string): Promise<string> {
    const backups = await this.listBackups();
    const backup = backups.find(b => b.id === backupId);
    
    if (!backup) {
      throw new Error(`Backup ${backupId} not found`);
    }

    const exportPath = path.join('./downloads', `export_${backup.name}`);
    fs.copyFileSync(backup.path, exportPath);
    
    return exportPath;
  }

  async importBackup(filePath: string, userId: string): Promise<BackupRecord> {
    const fileName = path.basename(filePath);
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const importedName = `imported_${timestamp}_${fileName}`;
    const importedPath = path.join(this.backupDir, importedName);
    
    fs.copyFileSync(filePath, importedPath);
    
    const stats = fs.statSync(importedPath);
    
    const backupRecord: BackupRecord = {
      id: timestamp,
      name: importedName,
      type: this.determineBackupType(fileName),
      description: `Imported backup from ${fileName}`,
      size: stats.size,
      createdAt: new Date(),
      createdBy: userId,
      path: importedPath,
      isCorrupted: await this.checkBackupIntegrity(importedPath)
    };

    await auditLogger.log({
      userId,
      module: 'backup',
      action: "backup_imported",
      entityType: "backup",
      entityId: importedName,
      newValues: { 
        originalFile: fileName,
        size: backupRecord.size,
        isCorrupted: backupRecord.isCorrupted
      },
      timestamp: new Date(),
      success: true,
      severity: 'info'
    });

    return backupRecord;
  }

  async deleteBackup(backupId: string, userId: string): Promise<boolean> {
    try {
      const backups = await this.listBackups();
      const backup = backups.find(b => b.id === backupId);
      
      if (!backup) {
        throw new Error(`Backup ${backupId} not found`);
      }

      fs.unlinkSync(backup.path);

      await auditLogger.log({
        userId,
        module: 'backup',
        action: "backup_deleted",
        entityType: "backup",
        entityId: backupId,
        oldValues: { 
          backupName: backup.name,
          backupType: backup.type,
          backupSize: backup.size
        },
        timestamp: new Date(),
        success: true,
        severity: 'info'
      });

      return true;
    } catch (error) {
      console.error("Error deleting backup:", error);
      throw new Error(`Failed to delete backup: ${error}`);
    }
  }

  private async generateMetadata() {
    // Get table count and record count from database
    // Get table count from the pool directly
    const tableCountResult = await pool.query(`
      SELECT COUNT(*) as count 
      FROM information_schema.tables 
      WHERE table_schema = 'public'
    `);
    
    const recordCountResult = await pool.query(`
      SELECT SUM(n_tup_ins + n_tup_upd) as total_records
      FROM pg_stat_user_tables
    `);

    const tableCount = parseInt(tableCountResult.rows[0]?.count || '0');
    const recordCount = parseInt(recordCountResult.rows[0]?.total_records || '0');
    
    // Generate simple checksum
    const checksum = this.generateChecksum(`${tableCount}-${recordCount}-${Date.now()}`);

    return {
      tableCount,
      recordCount,
      checksum
    };
  }

  private async getIncrementalChanges(since: Date) {
    // Get audit logs since the specified date
    const auditLogs = await storage.getAuditLogs(1, 10000, {
      startDate: since,
      endDate: new Date()
    });

    return {
      auditLogs: auditLogs.logs,
      totalChanges: auditLogs.logs.length,
      period: {
        from: since.toISOString(),
        to: new Date().toISOString()
      }
    };
  }

  private determineBackupType(fileName: string): 'full' | 'incremental' | 'rnc_only' {
    if (fileName.includes('incremental')) return 'incremental';
    if (fileName.includes('DGII') || fileName.includes('RNC')) return 'rnc_only';
    return 'full';
  }

  private async getBackupDescription(filePath: string): Promise<string> {
    try {
      const content = fs.readFileSync(filePath, 'utf8');
      const descMatch = content.match(/-- Description: (.+)/);
      return descMatch ? descMatch[1] : 'No description available';
    } catch {
      return 'Description unavailable';
    }
  }

  private async checkBackupIntegrity(filePath: string): Promise<boolean> {
    try {
      const stats = fs.statSync(filePath);
      
      // Basic integrity checks
      if (stats.size === 0) return false;
      
      const content = fs.readFileSync(filePath, 'utf8');
      
      if (filePath.endsWith('.sql')) {
        return content.includes('-- Four One Solutions ERP Database Backup');
      } else if (filePath.endsWith('.json')) {
        JSON.parse(content);
        return true;
      }
      
      return false;
    } catch {
      return false;
    }
  }

  private async restoreFullBackup(backupPath: string): Promise<void> {
    const restoreCommand = `psql "${process.env.DATABASE_URL}" < "${backupPath}"`;
    await execAsync(restoreCommand);
  }

  private async restoreIncrementalBackup(backupPath: string): Promise<void> {
    const content = fs.readFileSync(backupPath, 'utf8');
    const backup = JSON.parse(content);
    
    // Apply incremental changes
    console.log(`Applying ${backup.changes.totalChanges} incremental changes`);
    // Implementation would depend on the specific change format
  }

  private generateChecksum(data: string): string {
    let hash = 0;
    for (let i = 0; i < data.length; i++) {
      const char = data.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // Convert to 32-bit integer
    }
    return Math.abs(hash).toString(16);
  }
}

export const backupService = new BackupService();